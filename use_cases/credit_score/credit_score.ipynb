{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df64f867",
   "metadata": {},
   "source": [
    "<font size=6>Example: Credit Score Classification</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99a81f",
   "metadata": {},
   "source": [
    "<font size=2>`FeatureStore` is a model-agnostic tool aiming to help data scientists and algorithm engineers get rid of tiring data storing and merging tasks.\n",
    "<br>`FeatureStore` not only work on single-dimension data such as classification and prediction, but also work on time-series data.\n",
    " <br>After collecting data, all you need to do is config several straight-forward .yml files, then you can focus on  models/algorithms and leave all exhausting preparation to `FeatureStore`.</font>\n",
    " <font size=4><br><br>Here we present credit scoring mission as a single-dimension data demo, it takes features like wages, loan records to decide whether to grant credit or not.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebee68",
   "metadata": {},
   "source": [
    "<font size=4>Import packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc14dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tempfile\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from f2ai.featurestore import FeatureStore\n",
    "from f2ai.common.sampler import GroupFixednbrSampler\n",
    "from f2ai.common.collect_fn import classify_collet_fn\n",
    "from f2ai.common.utils import get_bucket_from_oss_url\n",
    "from f2ai.models.earlystop import EarlyStopping    \n",
    "from f2ai.models.sequential import SimpleClassify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14ac45",
   "metadata": {},
   "source": [
    "<font size=4>Download demo project files from `OSS` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda81c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project downloaded and saved in /tmp/558u5bgc/xyz_test_data\n"
     ]
    }
   ],
   "source": [
    "download_from = \"oss://aiexcelsior-shanghai-test/xyz_test_data/credit-score.zip\"\n",
    "save_path = '/tmp/'\n",
    "save_dir = tempfile.mkdtemp(prefix=save_path)\n",
    "bucket, key = get_bucket_from_oss_url(download_from)\n",
    "dest_zip_filepath = os.path.join(save_dir,key)\n",
    "os.makedirs(os.path.dirname(dest_zip_filepath), exist_ok=True)\n",
    "bucket.get_object_to_file(key, dest_zip_filepath)\n",
    "zipfile.ZipFile(dest_zip_filepath).extractall(dest_zip_filepath.rsplit('/',1)[0])\n",
    "os.remove(dest_zip_filepath)\n",
    "print(f\"Project downloaded and saved in {dest_zip_filepath.rsplit('/',1)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738d2da",
   "metadata": {},
   "source": [
    "<font size=4>Initialize `FeatureStore`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbdaf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_COL = 'event_timestamp'\n",
    "fs = FeatureStore(f\"file://{save_dir}/{key.rstrip('.zip')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbff283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features are: ['person_home_ownership', 'location_type', 'mortgage_due', 'tax_returns_filed', 'population', 'person_emp_length', 'loan_amnt', 'person_age', 'missed_payments_2y', 'state', 'total_wages', 'hard_pulls', 'loan_int_rate', 'missed_payments_6m', 'student_loan_due', 'city', 'person_income', 'loan_intent', 'bankruptcies', 'vehicle_loan_due', 'missed_payments_1y', 'credit_card_due']\n"
     ]
    }
   ],
   "source": [
    "print(f\"All features are: {fs._get_feature_to_use(fs.services['credit_scoring_v1'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ba5e1",
   "metadata": {},
   "source": [
    "<font size=4>Get the time range of available data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb8697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest timestamp: 2020-08-25 20:34:41.361000+00:00\n",
      "Latest timestamp: 2021-08-25 20:34:41.361000+00:00\n"
     ]
    }
   ],
   "source": [
    "print(f'Earliest timestamp: {fs.get_latest_entities(\"credit_scoring_v1\")[TIME_COL].min()}')\n",
    "print(f'Latest timestamp: {fs.get_latest_entities(\"credit_scoring_v1\")[TIME_COL].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13419833",
   "metadata": {},
   "source": [
    "<font size=4>Split the train / valid / test data at approximately 7/2/1, use `GroupFixednbrSampler` to downsample original data and return a `torch.IterableDataset`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec10ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = fs.get_dataset(\n",
    "        service=\"credit_scoring_v1\",\n",
    "        sampler=GroupFixednbrSampler(\n",
    "            time_bucket=\"5 days\",\n",
    "            stride=1,\n",
    "            group_ids=None,\n",
    "            group_names=None,\n",
    "            start=\"2020-08-20\",\n",
    "            end=\"2021-04-30\",\n",
    "        ),\n",
    "    )\n",
    "ds_valid = fs.get_dataset(\n",
    "        service=\"credit_scoring_v1\",\n",
    "        sampler=GroupFixednbrSampler(\n",
    "            time_bucket=\"5 days\",\n",
    "            stride=1,\n",
    "            group_ids=None,\n",
    "            group_names=None,\n",
    "            start=\"2021-04-30\",\n",
    "            end=\"2021-07-31\",\n",
    "        ),\n",
    "    )\n",
    "ds_test= fs.get_dataset(\n",
    "        service=\"credit_scoring_v1\",\n",
    "        sampler=GroupFixednbrSampler(\n",
    "            time_bucket=\"1 days\",\n",
    "            stride=1,\n",
    "            group_ids=None,\n",
    "            group_names=None,\n",
    "            start=\"2021-07-31\",\n",
    "            end=\"2021-08-31\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8d35f",
   "metadata": {},
   "source": [
    "<font size=4>Using `FeatureStore.stats` to obtain `statistical results` for data processing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5b9ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values of categorical features are: {'person_home_ownership': 4, 'location_type': 1, 'state': 51, 'city': 8166, 'loan_intent': 6}\n"
     ]
    }
   ],
   "source": [
    "# catgorical features\n",
    "features_cat = [  \n",
    "    fea\n",
    "    for fea in fs.services[\"credit_scoring_v1\"].get_feature_names(fs.feature_views)\n",
    "    if fea not in fs.services[\"credit_scoring_v1\"].get_feature_names(fs.feature_views,is_numeric=True)\n",
    "]\n",
    "# get unique item number to do labelencoder\n",
    "cat_unique = fs.stats(\n",
    "    \"credit_scoring_v1\",\n",
    "    fn=\"unique\",\n",
    "    group_key=[],\n",
    "    start=\"2020-08-01\",\n",
    "    end=\"2021-04-30\",\n",
    "    features=features_cat,\n",
    ").to_dict()\n",
    "cat_count = {key: len(cat_unique[key]) for key in cat_unique.keys()}\n",
    "print(f\"Number of unique values of categorical features are: {cat_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a0d6e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max boundary of continuous features are: {'population': [272.0, 88503.0], 'missed_payments_6m': [0.0, 1.0], 'person_age': [20.0, 144.0], 'missed_payments_2y': [0.0, 7.0], 'student_loan_due': [0.0, 49997.0], 'missed_payments_1y': [0.0, 3.0], 'bankruptcies': [0.0, 2.0], 'mortgage_due': [33.0, 1999896.0], 'vehicle_loan_due': [1.0, 29998.0], 'total_wages': [0.0, 2132869892.0], 'tax_returns_filed': [250.0, 47778.0], 'credit_card_due': [0.0, 9998.0], 'person_emp_length': [0.0, 41.0], 'hard_pulls': [0.0, 10.0], 'loan_int_rate': [5.42, 23.22], 'loan_amnt': [500.0, 35000.0], 'person_income': [4000.0, 6000000.0]}\n"
     ]
    }
   ],
   "source": [
    "# contiouns features \n",
    "cont_scalar_max = fs.stats(\n",
    "    \"credit_scoring_v1\", fn=\"max\", group_key=[], start=\"2020-08-01\", end=\"2021-04-30\"\n",
    ").to_dict()\n",
    "cont_scalar_min = fs.stats(\n",
    "    \"credit_scoring_v1\", fn=\"min\", group_key=[], start=\"2020-08-01\", end=\"2021-04-30\"\n",
    ").to_dict()\n",
    "cont_scalar = {key: [cont_scalar_min[key], cont_scalar_max[key]] for key in cont_scalar_min.keys()}\n",
    "print(f\"Min-Max boundary of continuous features are: {cont_scalar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a08f5",
   "metadata": {},
   "source": [
    "<font size=4>Construct `torch.DataLoader` from  `torch.IterableDataset` for modelling</font>\n",
    "<font size = 3><br>Here we compose data-preprocess in `collect_fn`, so the time range of `statistical results` used to `.fit()` should be corresponding to `train` data only so as to avoid information leakage. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1379abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "\n",
    "train_dataloader = DataLoader(  \n",
    "    ds_train.to_pytorch(),\n",
    "    collate_fn=lambda x: classify_collet_fn(\n",
    "        x,\n",
    "        cat_coder=cat_unique,\n",
    "        cont_scalar=cont_scalar,\n",
    "        label=fs._get_feature_to_use(fs.services[\"credit_scoring_v1\"].get_label_view(fs.label_views)),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valie_dataloader = DataLoader(  \n",
    "    ds_valid.to_pytorch(),\n",
    "    collate_fn=lambda x: classify_collet_fn(\n",
    "        x,\n",
    "        cat_coder=cat_unique,\n",
    "        cont_scalar=cont_scalar,\n",
    "        label=fs._get_feature_to_use(fs.services[\"credit_scoring_v1\"].get_label_view(fs.label_views)),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader( \n",
    "    ds_valid.to_pytorch(),\n",
    "    collate_fn=lambda x: classify_collet_fn(\n",
    "        x,\n",
    "        cat_coder=cat_unique,\n",
    "        cont_scalar=cont_scalar,\n",
    "        label=fs._get_feature_to_use(fs.services[\"credit_scoring_v1\"].get_label_view(fs.label_views)),\n",
    "    ),\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7cb71",
   "metadata": {},
   "source": [
    "<font size=4>Customize `model`, `optimizer` and `loss` function suitable to task</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a9eb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassify(\n",
    "    cont_nbr=len(cont_scalar_max), cat_nbr=len(cat_count), emd_dim=8, max_types=max(cat_count.values()),hidden_size=4\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "loss_fn = nn.BCELoss()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb956ec5",
   "metadata": {},
   "source": [
    "<font size=4>Use `train_dataloader` to train while `valie_dataloader` to guide `earlystop`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3321113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 done, train loss: 0.9228071888287862, valid loss: 0.8917452792326609\n",
      "epoch: 1 done, train loss: 0.8228938817977905, valid loss: 0.806285967429479\n",
      "epoch: 2 done, train loss: 0.7159536878267924, valid loss: 0.715582956870397\n",
      "epoch: 3 done, train loss: 0.6077397664388021, valid loss: 0.6279712617397308\n",
      "epoch: 4 done, train loss: 0.523370603720347, valid loss: 0.5640117128690084\n",
      "epoch: 5 done, train loss: 0.46912830471992495, valid loss: 0.530453751484553\n",
      "epoch: 6 done, train loss: 0.4288410405317942, valid loss: 0.5177373588085175\n",
      "epoch: 7 done, train loss: 0.4263931175072988, valid loss: 0.5144786983728409\n",
      "epoch: 8 done, train loss: 0.4069450835386912, valid loss: 0.5147957305113474\n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch: 9 done, train loss: 0.4173213442166646, valid loss: 0.5159803281227747\n",
      "EarlyStopping counter: 2 out of 5\n",
      "epoch: 10 done, train loss: 0.43354902466138207, valid loss: 0.5168851017951965\n",
      "EarlyStopping counter: 3 out of 5\n",
      "epoch: 11 done, train loss: 0.3854812800884247, valid loss: 0.5179851104815801\n",
      "EarlyStopping counter: 4 out of 5\n",
      "epoch: 12 done, train loss: 0.3868887344996134, valid loss: 0.5189551711082458\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Trigger earlystop, stop epoch at 12\n"
     ]
    }
   ],
   "source": [
    "# you can also use any ready-to-use training frame like ignite, pytorch-lightening...\n",
    "early_stop = EarlyStopping(save_path=f\"{save_dir}/{key.rstrip('.zip')}\",patience=5,delta=1e-6)\n",
    "for epoch in range(50):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    for x, y in train_dataloader:\n",
    "        pred_label = model(x)\n",
    "        true_label = y\n",
    "        loss = loss_fn(pred_label, true_label)\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    for x, y in valie_dataloader:\n",
    "        pred_label = model(x)\n",
    "        true_label = y\n",
    "        loss = loss_fn(pred_label, true_label)\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "    print(f\"epoch: {epoch} done, train loss: {np.mean(train_loss)}, valid loss: {np.mean(valid_loss)}\")\n",
    "    early_stop(np.mean(valid_loss),model)\n",
    "    if early_stop.early_stop:\n",
    "        print(f\"Trigger earlystop, stop epoch at {epoch}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ea7e",
   "metadata": {},
   "source": [
    "<font size=4>Get prediction result of `test_dataloader`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87afe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(f\"{save_dir}/{key.rstrip('.zip')}\",'best_chekpnt.pk'))\n",
    "model.eval()\n",
    "preds=[]\n",
    "trues=[]\n",
    "for x,y in test_dataloader:\n",
    "    pred = model(x)\n",
    "    pred_label = 1 if pred.cpu().detach().numpy() >0.5 else 0\n",
    "    preds.append(pred_label)\n",
    "    trues.append(y.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d939513",
   "metadata": {},
   "source": [
    "<font size =4>Model Evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88810eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7956989247311828\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "acc = [1 if preds[i]==trues[i] else 0 for i in range(len(trues))]\n",
    "print(f\"Accuracy: {np.sum(acc) / len(acc)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autonn-3.8.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5840e4ed671345474330e8fce6ab52c58896a3935f0e728b8dbef1ddfad82808"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
