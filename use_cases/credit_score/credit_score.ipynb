{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df64f867",
   "metadata": {},
   "source": [
    "<font size=6>Example: Credit Score Classification</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99a81f",
   "metadata": {},
   "source": [
    "<font size=2>`FeatureStore` is a model-agnostic tool aiming to help data scientists and algorithm engineers get rid of tiring data storing and merging tasks.\n",
    "<br>`FeatureStore` not only work on single-dimension data such as classification and prediction, but also work on time-series data.\n",
    " <br>After collecting data, all you need to do is config several straight-forward .yml files, then you can focus on  models/algorithms and leave all exhausting preparation to `FeatureStore`.</font>\n",
    " <font size=4><br><br>Here we present credit scoring mission as a single-dimension data demo, it takes features like wages, loan records to decide whether to grant credit or not.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebee68",
   "metadata": {},
   "source": [
    "<font size=4>Import packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc14dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tempfile\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from aie_feast.featurestore import FeatureStore\n",
    "from aie_feast.common.sampler import GroupFixednbrSampler\n",
    "from aie_feast.common.collect_fn import classify_collet_fn\n",
    "from aie_feast.common.utils import get_bucket_from_oss_url\n",
    "from aie_feast.models.earlystop import EarlyStopping    \n",
    "from aie_feast.models.sequential import SimpleClassify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14ac45",
   "metadata": {},
   "source": [
    "<font size=4>Download demo project files from `OSS` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda81c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project downloaded and saved in /tmp/nh869w4q/xyz_test_data\n"
     ]
    }
   ],
   "source": [
    "download_from = \"oss://aiexcelsior-shanghai-test/xyz_test_data/credit_score.zip\"\n",
    "save_path = '/tmp/'\n",
    "save_dir = tempfile.mkdtemp(prefix=save_path)\n",
    "bucket, key = get_bucket_from_oss_url(download_from)\n",
    "dest_zip_filepath = os.path.join(save_dir,key)\n",
    "os.makedirs(os.path.dirname(dest_zip_filepath), exist_ok=True)\n",
    "bucket.get_object_to_file(key, dest_zip_filepath)\n",
    "zipfile.ZipFile(dest_zip_filepath).extractall(dest_zip_filepath.rsplit('/',1)[0])\n",
    "os.remove(dest_zip_filepath)\n",
    "print(f\"Project downloaded and saved in {dest_zip_filepath.rsplit('/',1)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738d2da",
   "metadata": {},
   "source": [
    "<font size=4>Initialize `FeatureStore`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbdaf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_COL = 'event_timestamp'\n",
    "fs = FeatureStore(f\"file://{save_dir}/{key.rstrip('.zip')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cbff283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features are: ['person_income', 'person_emp_length', 'population', 'student_loan_due', 'loan_int_rate', 'person_age', 'missed_payments_6m', 'loan_amnt', 'missed_payments_1y', 'bankruptcies', 'hard_pulls', 'city', 'total_wages', 'loan_intent', 'person_home_ownership', 'vehicle_loan_due', 'location_type', 'tax_returns_filed', 'credit_card_due', 'state', 'missed_payments_2y', 'mortgage_due']\n"
     ]
    }
   ],
   "source": [
    "print(f\"All features are: {fs._get_available_features(fs.services['credit_scoring_v1'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ba5e1",
   "metadata": {},
   "source": [
    "<font size=4>Get the time range of available data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eb8697f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be FeatureViews, LabelViews or Service",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEarliest timestamp: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfs\u001b[38;5;241m.\u001b[39mget_latest_entities(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcredit_scoring_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)[TIME_COL]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatest timestamp: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfs\u001b[38;5;241m.\u001b[39mget_latest_entities(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcredit_scoring_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)[TIME_COL]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/featurestore.py:1083\u001b[0m, in \u001b[0;36mget_latest_entities\u001b[0;34m(self, view, entity)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_latest_entities\u001b[39m(\u001b[38;5;28mself\u001b[39m, view: \u001b[38;5;28mstr\u001b[39m, entity: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []):\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;124;03m\"\"\"get latest entity and its timestamp from a single FeatureViews/LabelViews or a materialzed Service\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \n\u001b[1;32m   1080\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;124;03m        views (List): view to look up\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m     view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_views(view)\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m entity:\n\u001b[1;32m   1085\u001b[0m         entity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_available_entity_names(view)\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/featurestore.py:107\u001b[0m, in \u001b[0;36mFeatureStore._get_available_entity_names\u001b[0;34m(self, view)\u001b[0m\n\u001b[1;32m    105\u001b[0m     entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(view\u001b[38;5;241m.\u001b[39mget_entities(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_views, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_views))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be FeatureViews, LabelViews or Service\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m entities\n",
      "\u001b[0;31mTypeError\u001b[0m: must be FeatureViews, LabelViews or Service"
     ]
    }
   ],
   "source": [
    "print(f'Earliest timestamp: {fs.get_latest_entities(\"credit_scoring_v1\")[TIME_COL].min()}')\n",
    "print(f'Latest timestamp: {fs.get_latest_entities(\"credit_scoring_v1\")[TIME_COL].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13419833",
   "metadata": {},
   "source": [
    "<font size=4>Split the train / valid / test data at approximately 7/2/1, use `GroupFixednbrSampler` to downsample original data and return a `torch.IterableDataset`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = fs.get_dataset(\n",
    "        service_name=\"credit_scoring_v1\",\n",
    "        sampler=GroupFixednbrSampler(\n",
    "            time_bucket=\"5 days\",\n",
    "            stride=1,\n",
    "            group_ids=None,\n",
    "            group_names=None,\n",
    "            start=\"2020-08-20\",\n",
    "            end=\"2021-04-30\",\n",
    "        ),\n",
    "    )\n",
    "ds_valid = fs.get_dataset(\n",
    "        service_name=\"credit_scoring_v1\",\n",
    "        sampler=GroupFixednbrSampler(\n",
    "            time_bucket=\"5 days\",\n",
    "            stride=1,\n",
    "            group_ids=None,\n",
    "            group_names=None,\n",
    "            start=\"2021-04-30\",\n",
    "            end=\"2021-07-31\",\n",
    "        ),\n",
    "    )\n",
    "ds_test= fs.get_dataset(\n",
    "        service_name=\"credit_scoring_v1\",\n",
    "        sampler=GroupFixednbrSampler(\n",
    "            time_bucket=\"1 days\",\n",
    "            stride=1,\n",
    "            group_ids=None,\n",
    "            group_names=None,\n",
    "            start=\"2021-07-31\",\n",
    "            end=\"2021-08-31\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8d35f",
   "metadata": {},
   "source": [
    "<font size=4>Using `FeatureStore.stats` to obtain `statistical results` for data processing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b9ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catgorical features\n",
    "features_cat = [  \n",
    "    fea\n",
    "    for fea in fs._get_available_features(fs.services[\"credit_scoring_v1\"])\n",
    "    if fea not in fs._get_available_features(fs.services[\"credit_scoring_v1\"], is_numeric=True)\n",
    "]\n",
    "# get unique item number to do labelencoder\n",
    "cat_unique = fs.stats(\n",
    "    \"credit_scoring_v1\",\n",
    "    fn=\"unique\",\n",
    "    group_key=[],\n",
    "    start=\"2020-08-01\",\n",
    "    end=\"2021-04-30\",\n",
    "    features=features_cat,\n",
    ").to_dict()\n",
    "cat_count = {key: len(cat_unique[key]) for key in cat_unique.keys()}\n",
    "print(f\"Number of unique values of categorical features are: {cat_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contiouns features \n",
    "cont_scalar_max = fs.stats(\n",
    "    \"credit_scoring_v1\", fn=\"max\", group_key=[], start=\"2020-08-01\", end=\"2021-04-30\"\n",
    ").to_dict()\n",
    "cont_scalar_min = fs.stats(\n",
    "    \"credit_scoring_v1\", fn=\"min\", group_key=[], start=\"2020-08-01\", end=\"2021-04-30\"\n",
    ").to_dict()\n",
    "cont_scalar = {key: [cont_scalar_min[key], cont_scalar_max[key]] for key in cont_scalar_min.keys()}\n",
    "print(f\"Min-Max boundary of continuous features are: {cont_scalar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a08f5",
   "metadata": {},
   "source": [
    "<font size=4>Construct `torch.DataLoader` from  `torch.IterableDataset` for modelling</font>\n",
    "<font size = 3><br>Here we compose data-preprocess in `collect_fn`, so the time range of `statistical results` used to `.fit()` should be corresponding to `train` data only so as to avoid information leakage. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "\n",
    "train_dataloader = DataLoader(  \n",
    "    ds_train.to_pytorch(),\n",
    "    collate_fn=lambda x: classify_collet_fn(\n",
    "        x,\n",
    "        cat_coder=cat_unique,\n",
    "        cont_scalar=cont_scalar,\n",
    "        label=fs._get_available_labels(fs.services[\"credit_scoring_v1\"]),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valie_dataloader = DataLoader(  \n",
    "    ds_valid.to_pytorch(),\n",
    "    collate_fn=lambda x: classify_collet_fn(\n",
    "        x,\n",
    "        cat_coder=cat_unique,\n",
    "        cont_scalar=cont_scalar,\n",
    "        label=fs._get_available_labels(fs.services[\"credit_scoring_v1\"]),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader( \n",
    "    ds_valid.to_pytorch(),\n",
    "    collate_fn=lambda x: classify_collet_fn(\n",
    "        x,\n",
    "        cat_coder=cat_unique,\n",
    "        cont_scalar=cont_scalar,\n",
    "        label=fs._get_available_labels(fs.services[\"credit_scoring_v1\"]),\n",
    "    ),\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7cb71",
   "metadata": {},
   "source": [
    "<font size=4>Customize `model`, `optimizer` and `loss` function suitable to task</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9eb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassify(\n",
    "    cont_nbr=len(cont_scalar_max), cat_nbr=len(cat_count), emd_dim=8, max_types=max(cat_count.values()),hidden_size=4\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "loss_fn = nn.BCELoss()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb956ec5",
   "metadata": {},
   "source": [
    "<font size=4>Use `train_dataloader` to train while `valie_dataloader` to guide `earlystop`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3321113",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m valie_dataloader:\n\u001b[1;32m     19\u001b[0m     pred_label \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     20\u001b[0m     true_label \u001b[38;5;241m=\u001b[39m y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/dataset/dataset.py:41\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity_index)):\n\u001b[0;32m---> 41\u001b[0m         data_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_sample[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data_sample\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/dataset/dataset.py:73\u001b[0m, in \u001b[0;36mIterableDataset.get_context\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     71\u001b[0m     tmp_result\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{QUERY_COL: TIME_COL}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# always merge on TIME_COL\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     tmp_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m label_views_pd \u001b[38;5;241m=\u001b[39m label_views_pd\u001b[38;5;241m.\u001b[39mmerge(tmp_result, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(entity\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m     75\u001b[0m label_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m features\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/featurestore.py:225\u001b[0m, in \u001b[0;36mFeatureStore.get_labels\u001b[0;34m(self, label_view, entity_df, include)\u001b[0m\n\u001b[1;32m    222\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_available_labels(label_view)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_store\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_point_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_store\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpgsql\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    227\u001b[0m     table_suffix \u001b[38;5;241m=\u001b[39m to_pgsql(entity_df, TMP_TBL, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_store)\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/featurestore.py:322\u001b[0m, in \u001b[0;36mFeatureStore._get_point_record\u001b[0;34m(self, view, entity_df, features, include)\u001b[0m\n\u001b[1;32m    320\u001b[0m         df \u001b[38;5;241m=\u001b[39m get_newest_record(df, TIME_COL, entity_names, CREATE_COL)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# `Service`, read from materialized table\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialize_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mTIME_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMATERIALIZE_TIME\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentity_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjoin_key_to_entity_names\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39mjoin_key_to_entity_names, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    328\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m entity_names \u001b[38;5;241m+\u001b[39m [TIME_COL, MATERIALIZE_TIME] \u001b[38;5;241m+\u001b[39m features]]\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/common/utils.py:41\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path, file_format, time_cols, entity_cols)\u001b[0m\n\u001b[1;32m     39\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path, parse_dates\u001b[38;5;241m=\u001b[39mtime_cols \u001b[38;5;28;01mif\u001b[39;00m time_cols \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m time_cols:\n\u001b[0;32m---> 41\u001b[0m     df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entity_cols:\n\u001b[1;32m     43\u001b[0m     df[entity_cols] \u001b[38;5;241m=\u001b[39m df[entity_cols]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/pandas/core/tools/datetimes.py:1022\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(tz)\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[0;32m-> 1022\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m   1024\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/pandas/core/tools/datetimes.py:188\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    184\u001b[0m cache_array \u001b[38;5;241m=\u001b[39m Series(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Perform a quicker unique check\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mshould_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cache_array\n\u001b[1;32m    191\u001b[0m     unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/pandas/core/tools/datetimes.py:150\u001b[0m, in \u001b[0;36mshould_cache\u001b[0;34m(arg, unique_share, check_count)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m unique_share \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_share must be in next bounds: (0; 1)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# We can't cache if the items are not hashable.\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     unique_elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_count\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/pandas/core/arrays/datetimes.py:640\u001b[0m, in \u001b[0;36mDatetimeArray.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    638\u001b[0m start_i \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m chunksize\n\u001b[1;32m    639\u001b[0m end_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m chunksize, length)\n\u001b[0;32m--> 640\u001b[0m converted \u001b[38;5;241m=\u001b[39m \u001b[43mints_to_pydatetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    642\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m converted\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# you can also use any ready-to-use training frame like ignite, pytorch-lightening...\n",
    "early_stop = EarlyStopping(save_path=f\"{save_dir}/{key.rstrip('.zip')}\",patience=5,delta=1e-6)\n",
    "for epoch in range(50):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    for x, y in train_dataloader:\n",
    "        pred_label = model(x)\n",
    "        true_label = y\n",
    "        loss = loss_fn(pred_label, true_label)\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    for x, y in valie_dataloader:\n",
    "        pred_label = model(x)\n",
    "        true_label = y\n",
    "        loss = loss_fn(pred_label, true_label)\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "    print(f\"epoch: {epoch} done, train loss: {np.mean(train_loss)}, valid loss: {np.mean(valid_loss)}\")\n",
    "    early_stop(np.mean(valid_loss),model)\n",
    "    if early_stop.early_stop:\n",
    "        print(f\"Trigger earlystop, stop epoch at {epoch}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ea7e",
   "metadata": {},
   "source": [
    "<font size=4>Get prediction result of `test_dataloader`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87afe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(f\"{save_dir}/{key.rstrip('.zip')}\",'best_chekpnt.pk'))\n",
    "model.eval()\n",
    "preds=[]\n",
    "trues=[]\n",
    "for x,y in test_dataloader:\n",
    "    pred = model(x)\n",
    "    pred_label = 1 if pred.cpu().detach().numpy() >0.5 else 0\n",
    "    preds.append(pred_label)\n",
    "    trues.append(y.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d939513",
   "metadata": {},
   "source": [
    "<font size =4>Model Evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88810eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "acc = [1 if preds[i]==trues[i] else 0 for i in range(len(trues))]\n",
    "print(f\"Accuracy: {np.sum(acc) / len(acc)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ceec5af5ba4b79ab0cdd8a0c0d8d3abfa7ed6d65accf05ad9be4e1157bb8d1a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('autonn-3.8.7': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
