{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df64f867",
   "metadata": {},
   "source": [
    "<font size=6>Example: Credit Score Classification</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99a81f",
   "metadata": {},
   "source": [
    "<font size=2>`FeatureStore` is a model-agnostic tool aiming to help data scientists and algorithm engineers get rid of tiring data storing and merging tasks.\n",
    "<br>`FeatureStore` not only work on single-dimension data such as classification and prediction, but also work on time-series data.\n",
    " <br>After collecting data, all you need to do is config several straight-forward .yml files, then you can focus on  models/algorithms and leave all exhausting preparation to `FeatureStore`.</font>\n",
    " <font size=4><br><br>Here we present credit scoring mission as a single-dimension data demo, it takes features like wages, loan records to decide whether to grant credit or not.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebee68",
   "metadata": {},
   "source": [
    "<font size=4>Import packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc14dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tempfile\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from aie_feast.featurestore import FeatureStore\n",
    "from aie_feast.common.sampler import GroupFixednbrSampler\n",
    "from aie_feast.common.collect_fn import classify_collet_fn\n",
    "from aie_feast.common.utils import get_bucket_from_oss_url\n",
    "from aie_feast.models.earlystop import EarlyStopping    \n",
    "from aie_feast.models.sequential import SimpleClassify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14ac45",
   "metadata": {},
   "source": [
    "<font size=4>Download demo project files from `OSS` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda81c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project downloaded and saved in /tmp/3fb3hehs/xyz_test_data\n"
     ]
    }
   ],
   "source": [
    "download_from = \"oss://aiexcelsior-shanghai-test/xyz_test_data/credit_score.zip\"\n",
    "save_path = '/tmp/'\n",
    "save_dir = tempfile.mkdtemp(prefix=save_path)\n",
    "bucket, key = get_bucket_from_oss_url(download_from)\n",
    "dest_zip_filepath = os.path.join(save_dir,key)\n",
    "os.makedirs(os.path.dirname(dest_zip_filepath), exist_ok=True)\n",
    "bucket.get_object_to_file(key, dest_zip_filepath)\n",
    "zipfile.ZipFile(dest_zip_filepath).extractall(dest_zip_filepath.rsplit('/',1)[0])\n",
    "os.remove(dest_zip_filepath)\n",
    "print(f\"Project downloaded and saved in {dest_zip_filepath.rsplit('/',1)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738d2da",
   "metadata": {},
   "source": [
    "<font size=4>Initialize `FeatureStore`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbdaf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_COL = 'event_timestamp'\n",
    "fs = FeatureStore(f\"file://{save_dir}/{key.rstrip('.zip')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbff283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features are: ['total_wages', 'student_loan_due', 'missed_payments_2y', 'city', 'loan_intent', 'state', 'credit_card_due', 'bankruptcies', 'person_age', 'loan_amnt', 'population', 'location_type', 'missed_payments_1y', 'loan_int_rate', 'vehicle_loan_due', 'person_income', 'mortgage_due', 'person_emp_length', 'tax_returns_filed', 'hard_pulls', 'missed_payments_6m', 'person_home_ownership']\n"
     ]
    }
   ],
   "source": [
    "print(f\"All features are: {fs._get_available_features(fs.services['credit_scoring_v1'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ba5e1",
   "metadata": {},
   "source": [
    "<font size=4>Get the time range of available data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb8697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest timestamp: 2020-08-25 20:34:41.361000+00:00\n",
      "Latest timestamp: 2021-08-25 20:34:41.361000+00:00\n"
     ]
    }
   ],
   "source": [
    "print(f'Earliest timestamp: {fs.get_latest_entities(fs.services[\"credit_scoring_v1\"])[TIME_COL].min()}')\n",
    "print(f'Latest timestamp: {fs.get_latest_entities(fs.services[\"credit_scoring_v1\"])[TIME_COL].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13419833",
   "metadata": {},
   "source": [
    "<font size=4>Split the train / valid / test data at approximately 7/2/1, use `GroupFixednbrSampler` to downsample original data and return a `torch.IterableDataset`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec10ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = fs.get_dataset(\n",
    "        service_name=\"credit_scoring_v1\",\n",
    "        sampler=GroupFixednbrSampler(\n",
    "            time_bucket=\"5 days\",\n",
    "            stride=1,\n",
    "            group_ids=None,\n",
    "            group_names=None,\n",
    "            start=\"2020-08-20\",\n",
    "            end=\"2021-04-30\",\n",
    "        ),\n",
    "    )\n",
    "ds_valid = fs.get_dataset(\n",
    "        service_name=\"credit_scoring_v1\",\n",
    "        sampler=GroupFixednbrSampler(\n",
    "            time_bucket=\"5 days\",\n",
    "            stride=1,\n",
    "            group_ids=None,\n",
    "            group_names=None,\n",
    "            start=\"2021-04-30\",\n",
    "            end=\"2021-07-31\",\n",
    "        ),\n",
    "    )\n",
    "ds_test= fs.get_dataset(\n",
    "        service_name=\"credit_scoring_v1\",\n",
    "        sampler=GroupFixednbrSampler(\n",
    "            time_bucket=\"1 days\",\n",
    "            stride=1,\n",
    "            group_ids=None,\n",
    "            group_names=None,\n",
    "            start=\"2021-07-31\",\n",
    "            end=\"2021-08-31\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8d35f",
   "metadata": {},
   "source": [
    "<font size=4>Using `FeatureStore.stats` to obtain `statistical results` for data processing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5b9ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values of categorical features are: {'city': 8166, 'loan_intent': 6, 'state': 51, 'location_type': 1, 'person_home_ownership': 4}\n"
     ]
    }
   ],
   "source": [
    "# catgorical features\n",
    "features_cat = [  \n",
    "    fea\n",
    "    for fea in fs._get_available_features(fs.services[\"credit_scoring_v1\"])\n",
    "    if fea not in fs._get_available_features(fs.services[\"credit_scoring_v1\"], is_numeric=True)\n",
    "]\n",
    "# get unique item number to do labelencoder\n",
    "cat_unique = fs.stats(\n",
    "    fs.services[\"credit_scoring_v1\"],\n",
    "    fn=\"unique\",\n",
    "    group_key=[],\n",
    "    start=\"2020-08-01\",\n",
    "    end=\"2021-04-30\",\n",
    "    features=features_cat,\n",
    ").to_dict()\n",
    "cat_count = {key: len(cat_unique[key]) for key in cat_unique.keys()}\n",
    "print(f\"Number of unique values of categorical features are: {cat_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a0d6e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max boundary of continuous features are: {'mortgage_due': [33.0, 1999896.0], 'credit_card_due': [0.0, 9998.0], 'total_wages': [0.0, 2132869892.0], 'person_emp_length': [0.0, 41.0], 'student_loan_due': [0.0, 49997.0], 'missed_payments_1y': [0.0, 3.0], 'bankruptcies': [0.0, 2.0], 'tax_returns_filed': [250.0, 47778.0], 'hard_pulls': [0.0, 10.0], 'person_age': [20.0, 144.0], 'missed_payments_2y': [0.0, 7.0], 'missed_payments_6m': [0.0, 1.0], 'loan_int_rate': [5.42, 23.22], 'loan_amnt': [500.0, 35000.0], 'vehicle_loan_due': [1.0, 29998.0], 'person_income': [4000.0, 6000000.0], 'population': [272.0, 88503.0]}\n"
     ]
    }
   ],
   "source": [
    "# contiouns features \n",
    "cont_scalar_max = fs.stats(\n",
    "    fs.services[\"credit_scoring_v1\"], fn=\"max\", group_key=[], start=\"2020-08-01\", end=\"2021-04-30\"\n",
    ").to_dict()\n",
    "cont_scalar_min = fs.stats(\n",
    "    fs.services[\"credit_scoring_v1\"], fn=\"min\", group_key=[], start=\"2020-08-01\", end=\"2021-04-30\"\n",
    ").to_dict()\n",
    "cont_scalar = {key: [cont_scalar_min[key], cont_scalar_max[key]] for key in cont_scalar_min.keys()}\n",
    "print(f\"Min-Max boundary of continuous features are: {cont_scalar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a08f5",
   "metadata": {},
   "source": [
    "<font size=4>Construct `torch.DataLoader` from  `torch.IterableDataset` for modelling</font>\n",
    "<font size = 3><br>Here we compose data-preprocess in `collect_fn`, so the time range of `statistical results` used to `.fit()` should be corresponding to `train` data only so as to avoid information leakage. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1379abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "\n",
    "train_dataloader = DataLoader(  \n",
    "    ds_train.to_pytorch(),\n",
    "    collate_fn=lambda x: classify_collet_fn(\n",
    "        x,\n",
    "        cat_coder=cat_unique,\n",
    "        cont_scalar=cont_scalar,\n",
    "        label=fs._get_available_labels(fs.services[\"credit_scoring_v1\"]),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valie_dataloader = DataLoader(  \n",
    "    ds_valid.to_pytorch(),\n",
    "    collate_fn=lambda x: classify_collet_fn(\n",
    "        x,\n",
    "        cat_coder=cat_unique,\n",
    "        cont_scalar=cont_scalar,\n",
    "        label=fs._get_available_labels(fs.services[\"credit_scoring_v1\"]),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader( \n",
    "    ds_valid.to_pytorch(),\n",
    "    collate_fn=lambda x: classify_collet_fn(\n",
    "        x,\n",
    "        cat_coder=cat_unique,\n",
    "        cont_scalar=cont_scalar,\n",
    "        label=fs._get_available_labels(fs.services[\"credit_scoring_v1\"]),\n",
    "    ),\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7cb71",
   "metadata": {},
   "source": [
    "<font size=4>Customize `model`, `optimizer` and `loss` function suitable to task</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a9eb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassify(\n",
    "    cont_nbr=len(cont_scalar_max), cat_nbr=len(cat_count), emd_dim=8, max_types=max(cat_count.values()),hidden_size=4\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "loss_fn = nn.BCELoss()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb956ec5",
   "metadata": {},
   "source": [
    "<font size=4>Use `train_dataloader` to train while `valie_dataloader` to guide `earlystop`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3321113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 done, train loss: 0.5936278502146403, valid loss: 0.5878020048141479\n",
      "epoch: 1 done, train loss: 0.5602891544500986, valid loss: 0.5608051598072052\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m valie_dataloader:\n\u001b[1;32m     19\u001b[0m     pred_label \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     20\u001b[0m     true_label \u001b[38;5;241m=\u001b[39m y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/dataset/dataset.py:41\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity_index)):\n\u001b[0;32m---> 41\u001b[0m         data_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_sample[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data_sample\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/dataset/dataset.py:69\u001b[0m, in \u001b[0;36mIterableDataset.get_context\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     67\u001b[0m     tmp_result\u001b[38;5;241m.\u001b[39mrename({QUERY_COL: TIME_COL}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# always merge on TIME_COL\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     tmp_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m label_views_pd \u001b[38;5;241m=\u001b[39m label_views_pd\u001b[38;5;241m.\u001b[39mmerge(tmp_result, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(entity\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m     71\u001b[0m label_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m features\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/featurestore.py:200\u001b[0m, in \u001b[0;36mFeatureStore.get_labels\u001b[0;34m(self, label_view, entity_df, include)\u001b[0m\n\u001b[1;32m    197\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_available_labels(label_view)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_point_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpgsql\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    202\u001b[0m     table_suffix \u001b[38;5;241m=\u001b[39m to_pgsql(entity_df, TMP_TBL, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/featurestore.py:290\u001b[0m, in \u001b[0;36mFeatureStore._get_point_record\u001b[0;34m(self, view, entity_df, features, include)\u001b[0m\n\u001b[1;32m    288\u001b[0m         df \u001b[38;5;241m=\u001b[39m get_newest_record(df, TIME_COL, entity_names, CREATE_COL)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# `Service`, read from materialized table\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialize_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mTIME_COL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMATERIALIZE_TIME\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mentity_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjoin_key_to_entity_names\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39mjoin_key_to_entity_names, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    296\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m entity_names \u001b[38;5;241m+\u001b[39m [TIME_COL, MATERIALIZE_TIME] \u001b[38;5;241m+\u001b[39m features]]\n",
      "File \u001b[0;32m~/Desktop/xyz_warehouse/gitlab/f2ai/aie_feast/common/utils.py:33\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path, file_format, time_cols, entity_cols)\u001b[0m\n\u001b[1;32m     30\u001b[0m     file_format \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_format\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_format\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtsv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     35\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39mtime_cols \u001b[38;5;28;01mif\u001b[39;00m time_cols \u001b[38;5;28;01melse\u001b[39;00m [])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    491\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/pandas/io/parquet.py:240\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    234\u001b[0m     path,\n\u001b[1;32m    235\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    236\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    237\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    238\u001b[0m )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    244\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_as_manager(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:2737\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties)\u001b[0m\n\u001b[1;32m   2730\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2731\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword is no longer supported with the new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2732\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets-based implementation. Specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to temporarily recover the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbehaviour.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2735\u001b[0m     )\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2737\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43m_ParquetDatasetV2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   2751\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   2752\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[1;32m   2753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/autonn2/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:2340\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, **kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2337\u001b[0m     fragment \u001b[38;5;241m=\u001b[39m parquet_format\u001b[38;5;241m.\u001b[39mmake_fragment(single_file, filesystem)\n\u001b[1;32m   2339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mFileSystemDataset(\n\u001b[0;32m-> 2340\u001b[0m         [fragment], schema\u001b[38;5;241m=\u001b[39mschema \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mfragment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphysical_schema\u001b[49m,\n\u001b[1;32m   2341\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparquet_format,\n\u001b[1;32m   2342\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfragment\u001b[38;5;241m.\u001b[39mfilesystem\n\u001b[1;32m   2343\u001b[0m     )\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;66;03m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# you can also use any ready-to-use training frame like ignite, pytorch-lightening...\n",
    "early_stop = EarlyStopping(save_path=f\"{save_dir}/{key.rstrip('.zip')}\",patience=5,delta=1e-6)\n",
    "for epoch in range(50):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    model.train()\n",
    "    for x, y in train_dataloader:\n",
    "        pred_label = model(x)\n",
    "        true_label = y\n",
    "        loss = loss_fn(pred_label, true_label)\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    for x, y in valie_dataloader:\n",
    "        pred_label = model(x)\n",
    "        true_label = y\n",
    "        loss = loss_fn(pred_label, true_label)\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "    print(f\"epoch: {epoch} done, train loss: {np.mean(train_loss)}, valid loss: {np.mean(valid_loss)}\")\n",
    "    early_stop(np.mean(valid_loss),model)\n",
    "    if early_stop.early_stop:\n",
    "        print(f\"Trigger earlystop, stop epoch at {epoch}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ea7e",
   "metadata": {},
   "source": [
    "<font size=4>Get prediction result of `test_dataloader`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87afe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(f\"{save_dir}/{key.rstrip('.zip')}\",'best_chekpnt.pk'))\n",
    "model.eval()\n",
    "preds=[]\n",
    "trues=[]\n",
    "for x,y in test_dataloader:\n",
    "    pred = model(x)\n",
    "    pred_label = 1 if pred.cpu().detach().numpy() >0.5 else 0\n",
    "    preds.append(pred_label)\n",
    "    trues.append(y.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d939513",
   "metadata": {},
   "source": [
    "<font size =4>Model Evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88810eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "acc = [1 if preds[i]==trues[i] else 0 for i in range(len(trues))]\n",
    "print(f\"Accuracy: {np.sum(acc) / len(acc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.removedirs(save_dir)\n",
    "print(f\"Project files removed\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ceec5af5ba4b79ab0cdd8a0c0d8d3abfa7ed6d65accf05ad9be4e1157bb8d1a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('autonn-3.8.7': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
